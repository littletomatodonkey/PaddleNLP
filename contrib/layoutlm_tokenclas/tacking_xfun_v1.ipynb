{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-09-01 05:28:52,744] [    INFO]\u001b[0m - Already cached ./.paddlenlp/models/ernie-gram-zh/vocab.txt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 18 extraneous bytes before marker 0xc4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: bad Huffman code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, \"/paddle/lcode/gry/PaddleNLP/\")\n",
    "from paddlenlp.transformers import ErnieGramTokenizer\n",
    "tokenizer = ErnieGramTokenizer.from_pretrained('ernie-gram-zh')\n",
    "\n",
    "lang = \"zh\"\n",
    "mode = \"val\"\n",
    "label_mode = \"test\"\n",
    "\n",
    "# mode = \"train\"\n",
    "# label_mode = \"train\"\n",
    "\n",
    "max_norm_size = 1000\n",
    "chunk_size = 509\n",
    "set_path = \"./data/XFUN_v1.0_data/\" + lang + \"/\"\n",
    "label_path = set_path + lang + \".\" + mode + \".json\"\n",
    "img_set_path = set_path + mode + \"/\"\n",
    "label_set_dict = {}\n",
    "\n",
    "save_label_path = set_path + label_mode + \".txt\"\n",
    "save_label_bbox_path = set_path + label_mode + \"_box.txt\"\n",
    "save_label_image_path = set_path + label_mode + \"_image.txt\"\n",
    "save_label_map_path = set_path + \"labels.txt\"\n",
    "foutl = open(save_label_path, \"wb\")\n",
    "foutb = open(save_label_bbox_path, \"wb\")\n",
    "fouti = open(save_label_image_path, \"wb\")\n",
    "foutlm = open(save_label_map_path, \"wb\")\n",
    "\n",
    "def get_norm_resize_bbox(box, img_height, img_width, max_norm_size=1000):\n",
    "    im_size_max = max(img_height, img_width)\n",
    "    im_scale = float(max_norm_size) / float(im_size_max)\n",
    "    box = np.array(box) * im_scale\n",
    "    box = [int(np.round(i)) for i in box]\n",
    "    return box\n",
    "\n",
    "with open(label_path, \"rb\") as fin:\n",
    "    lines = fin.readlines()\n",
    "    label_data = json.loads(lines[0])['documents']\n",
    "    sample_num = len(label_data)\n",
    "    for sno in range(sample_num):\n",
    "        if sno % 10 == 0:\n",
    "            print(sno)\n",
    "        img_id = label_data[sno]['id']\n",
    "        document = label_data[sno]['document']\n",
    "        bbox_num = len(document)\n",
    "        img_path = img_set_path + img_id + \".jpg\"\n",
    "        img = cv2.imread(img_path)\n",
    "        img_height, img_width = img.shape[0:2]\n",
    "        \n",
    "        tokens = []\n",
    "        token_boxes = []\n",
    "        actual_bboxes = []\n",
    "        label_ids = []\n",
    "        segment_ids = []\n",
    "        for bno in range(bbox_num):\n",
    "            label = document[bno]['label']\n",
    "            words = document[bno]['words']\n",
    "            words_num = len(words)\n",
    "            for wno in range(words_num):\n",
    "                word_text = words[wno]['text']\n",
    "                actual_bbox = words[wno]['box']\n",
    "                bbox = get_norm_resize_bbox(actual_bbox, img_height, img_width, max_norm_size)\n",
    "                word_tokens = tokenizer.tokenize(word_text)\n",
    "                tokens.extend(word_tokens)\n",
    "                token_boxes.extend([bbox] * len(word_tokens))\n",
    "                actual_bboxes.extend([actual_bbox] * len(word_tokens))\n",
    "                if label == \"other\":\n",
    "                    token_label = [\"O\"] * len(word_tokens)\n",
    "                else:\n",
    "                    token_label = [f\"I-{label.upper()}\"] * len(word_tokens)\n",
    "                    token_label[0] = f\"B-{label.upper()}\"\n",
    "                tmp_seg_idx = [bno] * len(word_tokens)\n",
    "                label_ids.extend(token_label)\n",
    "                segment_ids.extend(tmp_seg_idx)\n",
    "        \n",
    "        for chunk_id, index in enumerate(range(0, len(tokens), chunk_size)):\n",
    "            beg_no = chunk_id * chunk_size\n",
    "            end_no = min((chunk_id + 1) * chunk_size, len(tokens))\n",
    "            for tno in range(beg_no, end_no):\n",
    "                text = tokens[tno].strip(\"\\n\")\n",
    "                label = label_ids[tno].strip(\"\\n\")\n",
    "                bbox = token_boxes[tno]\n",
    "                actual_bbox = actual_bboxes[tno]\n",
    "                segment_id = segment_ids[tno]\n",
    "                otstr = \"%s\\t%s\\t%d\\t%d\\n\" % (text, label, sno, segment_id)\n",
    "                foutl.write(otstr.encode('utf-8'))\n",
    "\n",
    "                otstr = \"%s\\t%d %d %d %d\\t%d %d\\t%s\\n\" % (\n",
    "                    text, actual_bbox[0], actual_bbox[1], \n",
    "                    actual_bbox[2], actual_bbox[3],\n",
    "                    img_width, img_height, img_path)\n",
    "                fouti.write(otstr.encode('utf-8'))\n",
    "\n",
    "                otstr = \"%s\\t%d %d %d %d\\n\" % (\n",
    "                    text, bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "                foutb.write(otstr.encode('utf-8'))\n",
    "                label_set_dict[label] = 0\n",
    "                \n",
    "            otstr = \"\\n\"\n",
    "            foutl.write(otstr.encode('utf-8'))\n",
    "            foutb.write(otstr.encode('utf-8'))\n",
    "            fouti.write(otstr.encode('utf-8'))\n",
    "            \n",
    "for label in label_set_dict:\n",
    "    otstr = \"%s\\n\" % label\n",
    "    foutlm.write(otstr.encode('utf-8'))\n",
    "foutl.close()\n",
    "foutb.close()\n",
    "fouti.close()\n",
    "foutlm.close()\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
